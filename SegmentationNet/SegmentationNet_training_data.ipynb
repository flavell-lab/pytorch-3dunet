{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Network Training\n",
    "\n",
    "The original Segmentation 3D-UNet was trained on twelve original training images from non-NeuroPAL strains SWF358, SWF359, SWF360, and SWF366. This notebook takes as input the raw images and labels, and crops, resizes, and formats them to be compatible with the segmentation network.\n",
    "\n",
    "The input images are expected to be an image and a label file of the same shape and voxel size. The exact shape and voxel size do not matter as the notebook can automatically crop and resample the images as appropriate. The label images should take on an integer value between 0 and 3, as follows:\n",
    "\n",
    "- **0** represents unlabeled pixels. These will be assigned a weight of 0 during training, effectively causing them to be ignored.\n",
    "- **1** represents foreground pixels (aka neurons). They are assigned a high weight during training.\n",
    "- **2** represents background pixels (aka pixels that do not belong to any neuron). They are assigned a low weight during training, with the weight decreasing the further away they are from neurons.\n",
    "- **3** represents gap pixels. These are pixels between two adjacent neurons. In order for the Segmentation Net to learn to perform instance segmentation (splitting adjacent neurons into two separate objects), it is imperative that pixels between two adjacent neurons be labeled as background. Thus, those pixels are labeled with this special label rather than the typical background label 2, and when the network input data is created, they are assigned a very high weight.\n",
    "\n",
    "To label an image, you can load the image in ITK-SNAP from its MHD file via File -> Open Main Image. Then you can load the labels from their NRRD files via Segmentation -> Load Segmentation. You can then edit the segmentation in ITK-SNAP and save it to an NRRD file. Alternatively, to create an entirely new segmentation, you can simply start drawing on the image using the various label markers (as described above), and then save the result as an NRRD file. Remember to draw in all three dimensions! It is not necessary to label every pixel - indeed, in the initial twelve training and validation images, only about 1% of pixels had a nonzero label. Focus on labeling difficult pixels where the Segmentation Net is struggling - for example, very dim neurons, or a tight ball of neurons.\n",
    "\n",
    "The input images had the following original properties:\n",
    "- Images 1 through 14 have voxel size $0.36 \\times 0.36 \\times 1.0$. They were not used during training as 1.0 is too large to label gaps in the z-dimension competently. They are from a variety of different strains.\n",
    "- Images 15 through 19 have voxel size $0.36 \\times 0.36 \\times 0.2$. They are from SWF358-360 strains. They are unusually high SNR, so it is recommended to reduce their SNR before adding them to training data. This notebook does that automatically.\n",
    "- Images 20 through 22 have voxel size $0.36 \\times 0.36 \\times 0.36$. They were not used during training due to unknown reasons. Their strain information is unknown.\n",
    "- Images 23 through 29 have voxel size $0.54 \\times 0.54 \\times 0.54$, the same size as is currently standard for use in our lab. They are from the SWF366 strain.\n",
    "\n",
    "All of the images in the directories associated with this notebook have **already been pre-processed** to voxel size $0.54 \\times 0.54 \\times 0.54$ for training (see the `bin_images.ipynb` notebook for more details).\n",
    "\n",
    "The images 15, 16, 17, 18, 23, 25, 26, 27, and 29 were used for training while 19, 24, and 28 were used for validation.\n",
    "\n",
    "Eric later added four NeuroPAL animals to the training data to create the second version of the Segmentation Net. This consisted of 4 additional images (3 training and 1 validation). These labels were created by manual curation of the output of the first version of Segmentation Net, but they were not passed through this notebook and had all pixels weighted identically.\n",
    "\n",
    "The original segmentation network crop size was $210\\times96\\times51$, as this was the largest size that would fit on the 8GB GPUs we had available then, and there were some images with $x$-dimension of only 210. These images have since been padded, so that larger crop sizes will be possible.\n",
    "\n",
    "For more information, see https://github.com/flavell-lab/SegmentationTools.jl and https://github.com/flavell-lab/pytorch-3dunet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "using ImageDataIO, SegmentationTools, ProgressMeter, FlavellBase, MHDIO, WormFeatureDetector, HDF5, FileIO, NRRDIO, Statistics, StatsBase, Distributions\n",
    "using Plots, PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training and validation dataset paths\n",
    "\n",
    "Here, you can set which images to process and the relevant paths.\n",
    "\n",
    "The data is available in [our Dropbox](https://www.dropbox.com/scl/fo/ealblchspq427pfmhtg7h/ALZ7AE5o3bT0VUQ8TTeR1As?rlkey=1e6tseyuwd04rbj7wmn2n6ij7&e=2&st=ybsvv0ry&dl=0) under `SegmentationNet`. Set `rootpath` to the location of this data on your local machine. Please copy the data from the Dropbox before running this notebook to avoid modifying the contents of the Dropbox.\n",
    "\n",
    "This notebook only deals with the non-NeuroPAL images. The NeuroPAL ones were processed separately and their H5 files are available in `SegmentationNet/hdf5_train` and `SegmentationNet/hdf5_val` with non-integer filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "imgs = [15,16,17,18,19,23,24,25,26,27,28,29] # which datasets to use during training/validation\n",
    "rootpath=\"/store1/adam/test\" # root path to all input and output data.\n",
    "label_dir = \"label_binned_uncropped\" # subpath containing labels\n",
    "raw_dir = \"img_binned_uncropped\" # subpath containing images\n",
    "hdf5_dir = \"hdf5\"; # subpath containing output path where `pytorch-3dunet`-compatible H5 files will be written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "fg=5 # weight of foreground (neuron) labels\n",
    "gap=20 # base weight of background-gap labels\n",
    "scale_bkg_gap=true; # whether to increase the background-gap weight based on the number of adjacent neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to update these cropping parameters to your desired crop size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "crop_dict = Dict(\n",
    "    15 => [67:279, 139:234, 21:71],\n",
    "    16 => [33:245, 71:166, 21:71],\n",
    "    17 => [19:231, 147:242, 1:51],\n",
    "    18 => [141:236, 125:337, 21:71],\n",
    "    19 => [117:212, 83:295, 11:61],\n",
    "    20 => [195:338, 135:454, 1:77],\n",
    "    21 => [175:318, 135:454, 1:77],\n",
    "    22 => [185:328, 135:454, 1:77],\n",
    "    23 => [100:195, 57:266, 32:82],\n",
    "    24 => [61:270, 70:165, 21:71],\n",
    "    25 => [61:270, 60:155, 34:84],\n",
    "    26 => [51:260, 40:135, 28:78],\n",
    "    27 => [51:260, 40:135, 36:86],\n",
    "    28 => [51:260, 55:150, 40:90],\n",
    "    29 => [105:200, 57:266, 40:90]\n",
    "); # crop size parameters for all binned datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data\n",
    "\n",
    "This code crops the images and labels, and generates `pytorch-3dunet`-compatible training and validation datasets with image, label, and weight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "@showprogress for i in imgs\n",
    "    crop = crop_dict[i]\n",
    "\n",
    "    bin_scale=[1,1,1]\n",
    "    reduction_factor=1\n",
    "    if i in 15:19\n",
    "        bin_scale = [1,1,1] # the binning already happened\n",
    "        reduction_factor = 2\n",
    "    elseif i in 20:22\n",
    "        bin_scale = [1.5, 1.5, 1.5]\n",
    "        reduction_factor = 1\n",
    "    elseif i < 15\n",
    "        bin_scale = [1.5, 1.5, 0.54]\n",
    "        reduction_factor = 1\n",
    "    end\n",
    "    \n",
    "    transpose = (i in [10,11,12,13,14,18,19,20,21,22,23,29])\n",
    "\n",
    "    img = MHDIO.read_img(MHD(joinpath(rootpath, raw_dir, \"$(i)_img.mhd\")))\n",
    "    label = NRRDIO.read_img(NRRD(joinpath(rootpath, label_dir, \"$(i)_label.nrrd\")))\n",
    "\n",
    "\n",
    "    h5_dir = joinpath(rootpath, hdf5_dir)\n",
    "    create_dir(h5_dir)\n",
    "    \n",
    "    make_unet_input_h5(img, label, joinpath(h5_dir, string(i, pad=2)*\".h5\"), scale_xy=0.36, scale_z=1,\n",
    "        scale_bkg_gap=scale_bkg_gap, crop=crop, transpose=transpose, weight_foreground=fg, weight_bkg_gap=gap, bin_scale=bin_scale,\n",
    "        SN_reduction_factor=reduction_factor\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy files into training and validation directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "    train_dir = joinpath(rootpath, \"hdf5_train\")\n",
    "    create_dir(train_dir)\n",
    "    for i in [15,16,17,18,23,25,26,27,29] # training datasets\n",
    "        cp(joinpath(rootpath, hdf5_dir, \"$(i).h5\"), joinpath(train_dir, \"$(i).h5\"))\n",
    "    end\n",
    "    val_dir = joinpath(rootpath, \"hdf5_val\")\n",
    "    create_dir(val_dir)\n",
    "    for i in [19,24,28] # validation datasets\n",
    "        cp(joinpath(rootpath, hdf5_dir, \"$(i).h5\"), joinpath(val_dir, \"$(i).h5\"))\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "7b192ed86ce94536bf419698b7840590",
   "lastKernelId": "447d8ffd-98cc-4f7c-8c9b-74670b13ed98"
  },
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "name": "",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
